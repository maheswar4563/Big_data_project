{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92bb5805-f6f9-42cc-80d2-d2b7e4366f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Student Health Data').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed175afa-8039-4d70-af7b-2ca8fc28d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv('Student_Health_Data.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ff38a36-44a4-4eb3-8a2f-0f446967c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('student_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db2f2592-9212-42bf-b061-65e4b656c48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------+---------------------+-----------------+\n",
      "|avg_biosensor_stress|avg_self_reported_stress|avg_physical_activity|avg_sleep_quality|\n",
      "+--------------------+------------------------+---------------------+-----------------+\n",
      "|   5.483909405211377|       5.361600567638264|                1.905|            2.263|\n",
      "+--------------------+------------------------+---------------------+-----------------+\n",
      "\n",
      "Processing Time: 1.59 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "g1=spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        AVG(Stress_Level_Biosensor) AS avg_biosensor_stress,\n",
    "        AVG(Stress_Level_Self_Report) AS avg_self_reported_stress,\n",
    "        AVG(CASE \n",
    "            WHEN Physical_Activity = 'High' THEN 3\n",
    "            WHEN Physical_Activity = 'Moderate' THEN 2\n",
    "            ELSE 1 END) AS avg_physical_activity,\n",
    "        AVG(CASE \n",
    "            WHEN Sleep_Quality = 'Good' THEN 3\n",
    "            WHEN Sleep_Quality = 'Moderate' THEN 2\n",
    "            ELSE 1 END) AS avg_sleep_quality\n",
    "    FROM student_data\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "g1.show()\n",
    "end_time = time.time()\n",
    "processing_time = end_time - start_time\n",
    "print(f\"Processing Time: {processing_time:.2f} seconds\")\n",
    "g1.write.csv('Goal1',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01540517-5a01-401b-8857-82e7e1f2a723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|Health_Risk_Level|count|\n",
      "+-----------------+-----+\n",
      "|             High|  138|\n",
      "|              Low|  190|\n",
      "|         Moderate|  672|\n",
      "+-----------------+-----+\n",
      "\n",
      "Processing Time: 0.77 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "g2=spark.sql(\"\"\"\n",
    "    SELECT Health_Risk_Level, COUNT(*) AS count\n",
    "    FROM student_data\n",
    "    GROUP BY Health_Risk_Level\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "g2.show()\n",
    "end_time = time.time()\n",
    "processing_time = end_time - start_time\n",
    "print(f\"Processing Time: {processing_time:.2f} seconds\")\n",
    "g2.write.csv('Goal2',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4aaa2c5-dd6f-431e-962a-9d7e50c6796d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+------------------------+---------------------+------------------+\n",
      "|Health_Risk_Level|avg_biosensor_stress|avg_self_reported_stress|avg_physical_activity| avg_sleep_quality|\n",
      "+-----------------+--------------------+------------------------+---------------------+------------------+\n",
      "|             High|   7.513085120395608|       7.191850494337831|   2.3043478260869565|1.8840579710144927|\n",
      "|              Low|   2.999760089834212|      2.9657585555641286|    1.868421052631579|2.3684210526315788|\n",
      "|         Moderate|     5.7695673237028|       5.663141478961978|   1.8333333333333333|2.3110119047619047|\n",
      "+-----------------+--------------------+------------------------+---------------------+------------------+\n",
      "\n",
      "Processing Time: 1.47 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "g3=spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Health_Risk_Level,\n",
    "        AVG(Stress_Level_Biosensor) AS avg_biosensor_stress,\n",
    "        AVG(Stress_Level_Self_Report) AS avg_self_reported_stress,\n",
    "        AVG(CASE \n",
    "            WHEN Physical_Activity = 'High' THEN 3\n",
    "            WHEN Physical_Activity = 'Moderate' THEN 2\n",
    "            ELSE 1 END) AS avg_physical_activity,\n",
    "        AVG(CASE \n",
    "            WHEN Sleep_Quality = 'Good' THEN 3\n",
    "            WHEN Sleep_Quality = 'Moderate' THEN 2\n",
    "            ELSE 1 END) AS avg_sleep_quality\n",
    "    FROM student_data\n",
    "    GROUP BY Health_Risk_Level\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "g3.show()\n",
    "end_time = time.time()\n",
    "processing_time = end_time - start_time\n",
    "print(f\"Processing Time: {processing_time:.2f} seconds\")\n",
    "g3.write.csv('Goal3',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26890d02-d303-4240-ac61-b3401be3c978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------------------+------------------+\n",
      "|    Mood|avg_biosensor_stress|avg_self_reported_stress| avg_sleep_quality|\n",
      "+--------+--------------------+------------------------+------------------+\n",
      "|Stressed|   5.527092675450389|        5.35771862832382|2.3118279569892475|\n",
      "| Neutral|   5.462382357385251|       5.274956134294667| 2.255421686746988|\n",
      "|   Happy|   5.486169146021864|       5.453529090320141|2.2481203007518795|\n",
      "+--------+--------------------+------------------------+------------------+\n",
      "\n",
      "Processing Time: 1.15 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "g4=spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Mood,\n",
    "        AVG(Stress_Level_Biosensor) AS avg_biosensor_stress,\n",
    "        AVG(Stress_Level_Self_Report) AS avg_self_reported_stress,\n",
    "        AVG(CASE \n",
    "            WHEN Sleep_Quality = 'Good' THEN 3\n",
    "            WHEN Sleep_Quality = 'Moderate' THEN 2\n",
    "            ELSE 1 END) AS avg_sleep_quality\n",
    "    FROM student_data\n",
    "    GROUP BY Mood\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "g4.show()\n",
    "end_time = time.time()\n",
    "processing_time = end_time - start_time\n",
    "print(f\"Processing Time: {processing_time:.2f} seconds\")\n",
    "g4.write.csv('Goal4',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86019d93-eaf7-4254-b44e-48f49113be04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+------------------------+\n",
      "|   avg_study_hours| avg_project_hours|avg_biosensor_stress|avg_self_reported_stress|\n",
      "+------------------+------------------+--------------------+------------------------+\n",
      "|30.227037157426228|14.887368383808754|   5.483909405211377|       5.361600567638264|\n",
      "+------------------+------------------+--------------------+------------------------+\n",
      "\n",
      "Processing Time: 0.70 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "g5=spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        AVG(Study_Hours) AS avg_study_hours,\n",
    "        AVG(Project_Hours) AS avg_project_hours,\n",
    "        AVG(Stress_Level_Biosensor) AS avg_biosensor_stress,\n",
    "        AVG(Stress_Level_Self_Report) AS avg_self_reported_stress\n",
    "    FROM student_data\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "g5.show()\n",
    "end_time = time.time()\n",
    "processing_time = end_time - start_time\n",
    "print(f\"Processing Time: {processing_time:.2f} seconds\")\n",
    "g5.write.csv('Goal5',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f23fe714-d05f-497e-b05b-bdc00de13dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+------------------------+\n",
      "|   avg_study_hours| avg_project_hours|avg_biosensor_stress|avg_self_reported_stress|\n",
      "+------------------+------------------+--------------------+------------------------+\n",
      "|30.227037157426228|14.887368383808754|   5.483909405211377|       5.361600567638264|\n",
      "+------------------+------------------+--------------------+------------------------+\n",
      "\n",
      "Processing Time: 0.54 seconds\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/C:/Users/s562045/Downloads/Goal5 already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m processing_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocessing_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mg5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGoal5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\big-data-venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1863\u001b[0m )\n\u001b[1;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\big-data-venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\big-data-venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/C:/Users/s562045/Downloads/Goal5 already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "g5=spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        AVG(Study_Hours) AS avg_study_hours,\n",
    "        AVG(Project_Hours) AS avg_project_hours,\n",
    "        AVG(Stress_Level_Biosensor) AS avg_biosensor_stress,\n",
    "        AVG(Stress_Level_Self_Report) AS avg_self_reported_stress\n",
    "    FROM student_data\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "g5.show()\n",
    "end_time = time.time()\n",
    "processing_time = end_time - start_time\n",
    "print(f\"Processing Time: {processing_time:.2f} seconds\")\n",
    "g5.write.csv('Goal5',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78670b1e-f16b-4788-987f-3645e8904e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
